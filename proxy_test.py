import requests
from concurrent.futures import ThreadPoolExecutor
import json
import os
import re
from typing import List
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ä»£ç†æº URL åˆ—è¡¨ (ä½¿ç”¨å¯ç›´æ¥è®¿é—®çš„å…è´¹ä»£ç†åˆ—è¡¨)
PROXY_SOURCES = [
    "https://www.proxy-list.download/api/v1/get?type=http",
    "https://www.proxy-list.download/api/v1/get?type=https",
    "https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all",
    "https://api.proxyscrape.com/v2/?request=getproxies&protocol=socks4&timeout=10000&country=all",
    "https://api.proxyscrape.com/v2/?request=getproxies&protocol=socks5&timeout=10000&country=all",
]

# æµ‹è¯•ä»£ç†å¯ç”¨æ€§çš„ URL
TEST_URL = "http://httpbin.org/ip"

# æœ¬åœ°ä¿å­˜æœ‰æ•ˆä»£ç†çš„æ–‡ä»¶
PROXY_FILE = "proxies.json"

# è¯·æ±‚å¤´éƒ¨ä¿¡æ¯
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
}


# ä»£ç†æ± å­˜å‚¨ç±»
class ProxyStorage:
    def __init__(self):
        self.proxies = set()
        if os.path.exists(PROXY_FILE):
            try:
                with open(PROXY_FILE, "r", encoding="utf-8") as f:
                    self.proxies = set(json.load(f))
                logger.info(f"Loaded {len(self.proxies)} proxies from {PROXY_FILE}")
            except Exception as e:
                logger.error(f"Error loading proxies from file: {e}")
                self.proxies = set()

    def add(self, proxy: str):
        if proxy not in self.proxies:
            self.proxies.add(proxy)
            # å®æ—¶è¿½åŠ åˆ°æ–‡ä»¶ä¸­
            try:
                with open(PROXY_FILE, "r+", encoding="utf-8") as f:
                    try:
                        existing_proxies = json.load(f)
                    except:
                        existing_proxies = []

                    if proxy not in existing_proxies:
                        existing_proxies.append(proxy)
                        f.seek(0)
                        f.truncate()
                        json.dump(existing_proxies, f, ensure_ascii=False, indent=2)
            except FileNotFoundError:
                with open(PROXY_FILE, "w", encoding="utf-8") as f:
                    json.dump([proxy], f, ensure_ascii=False, indent=2)
            except Exception as e:
                logger.error(f"Error appending proxy to file: {e}")

    def remove(self, proxy: str):
        self.proxies.discard(proxy)

    def all(self):
        return list(self.proxies)

    def save(self):
        try:
            with open(PROXY_FILE, "w", encoding="utf-8") as f:
                json.dump(list(self.proxies), f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(self.proxies)} proxies to {PROXY_FILE}")
        except Exception as e:
            logger.error(f"Error saving proxies to file: {e}")


storage = ProxyStorage()


# æŠ“å–ä»£ç†åˆ—è¡¨
def fetch_proxies() -> List[str]:
    proxies = []
    for url in PROXY_SOURCES:
        try:
            logger.info(f"Fetching proxies from {url}")
            resp = requests.get(url, headers=HEADERS, timeout=15)
            resp.raise_for_status()
            # è§£æä»£ç†åˆ—è¡¨
            parsed_proxies = parse_proxies(resp.text)
            proxies.extend(parsed_proxies)
            logger.info(f"Got {len(parsed_proxies)} proxies from {url}")
        except Exception as e:
            logger.error(f"Failed to fetch {url}: {e}")
    return list(set(proxies))  # å»é‡


# è§£æä»£ç†åˆ—è¡¨
def parse_proxies(page_content: str) -> List[str]:
    proxies = []
    # åŒ¹é… IP:Port æ ¼å¼çš„ä»£ç†
    proxy_pattern = re.compile(r"\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}:[0-9]{1,5}\b")
    matches = proxy_pattern.findall(page_content)

    for match in matches:
        # ç®€å•éªŒè¯IPåœ°å€æ ¼å¼
        ip, port = match.split(":")
        if all(0 <= int(x) <= 255 for x in ip.split(".")):
            proxies.append(f"http://{match}")
            proxies.append(f"https://{match}")

    return list(set(proxies))  # å»é‡


# éªŒè¯ä»£ç†å¯ç”¨æ€§
def check_proxy(proxy: str) -> bool:
    try:
        proxies_dict = {"http": proxy, "https": proxy}
        resp = requests.get(TEST_URL, proxies=proxies_dict, timeout=10, headers=HEADERS)
        if resp.status_code == 200:
            logger.info(f"âœ… Valid proxy: {proxy}")
            storage.add(proxy)  # å·²ç»åœ¨ ProxyStorage.add ä¸­å®ç°äº†å®æ—¶è¿½åŠ åˆ°æ–‡ä»¶
            return True
    except requests.exceptions.Timeout:
        logger.warning(f"â° Timeout: {proxy}")
    except requests.exceptions.ConnectionError:
        logger.warning(f"ğŸ”Œ Connection error: {proxy}")
    except requests.exceptions.RequestException as e:
        logger.warning(f"âŒ Request error: {proxy}, Error: {e}")
    except Exception as e:
        logger.warning(f"âŒ Failed: {proxy}, Error: {e}")
    return False


# éªŒè¯æ‰€æœ‰ä»£ç†çš„å¯ç”¨æ€§
def validate_proxies(proxies: List[str]):
    logger.info(f"Validating {len(proxies)} proxies...")
    with ThreadPoolExecutor(max_workers=100) as executor:
        results = list(executor.map(check_proxy, proxies))
    valid_count = sum(results)
    logger.info(f"Validation complete. {valid_count}/{len(proxies)} proxies are valid.")


# ä¸»ç¨‹åº
if __name__ == "__main__":
    logger.info("ğŸ”„ Starting proxy fetching...")
    proxies = fetch_proxies()
    logger.info(f"âœ… Fetching complete, got {len(proxies)} unique proxies")

    if proxies:
        logger.info("ğŸ”„ Starting proxy validation...")
        validate_proxies(proxies)
        storage.save()
        logger.info(f"ğŸ’¾ Saved {len(storage.all())} valid proxies to {PROXY_FILE}")
    else:
        logger.warning("âš ï¸ No proxies fetched")
